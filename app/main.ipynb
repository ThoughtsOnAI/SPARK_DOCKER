{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import Window as W\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pg8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://postgres:5432/yourdbname\"\n",
    "db_config = {\n",
    "    'host': 'postgres',\n",
    "    'port': 5432,\n",
    "    'user': 'yourusername',\n",
    "    'password': 'yourpassword',\n",
    "    'database': 'yourdbname'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_spark_df_to_postgres(df, table_name='data_table', db_config=db_config):\n",
    "  pandas_df = df.toPandas()\n",
    "\n",
    "  conn = pg8000.connect(**db_config)\n",
    "  cursor = conn.cursor()\n",
    "\n",
    "  columns = pandas_df.columns.tolist()\n",
    "  types = pandas_df.dtypes.tolist()\n",
    "\n",
    "  # Map Pandas dtypes to PostgreSQL data types\n",
    "  type_mapping = {\n",
    "      'object': 'TEXT',\n",
    "      'int64': 'INTEGER',\n",
    "      'float64': 'FLOAT',\n",
    "      'datetime64[ns]': 'TIMESTAMP',\n",
    "      'bool': 'BOOLEAN',\n",
    "  }\n",
    "\n",
    "  # Build the CREATE TABLE statement\n",
    "  create_table_sql = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "  for col, dtype in zip(columns, types):\n",
    "      sql_type = type_mapping.get(str(dtype), 'TEXT')\n",
    "      create_table_sql += f\"{col} {sql_type}, \"\n",
    "  create_table_sql = create_table_sql.rstrip(', ') + ');'\n",
    "\n",
    "  cursor.execute(create_table_sql)\n",
    "\n",
    "  # Prepare the INSERT statement\n",
    "  placeholders = ', '.join(['%s'] * len(columns))\n",
    "  insert_sql = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders});\"\n",
    "  data_tuples = list(pandas_df.itertuples(index=False, name=None))\n",
    "\n",
    "  cursor.executemany(insert_sql, data_tuples)\n",
    "\n",
    "  conn.commit()\n",
    "  cursor.close()\n",
    "  conn.close()\n",
    "\n",
    "def read_postgres_to_spark_df(table_name='data_table', db_config=db_config):\n",
    "    conn = pg8000.connect(**db_config)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"SELECT * FROM {table_name};\")\n",
    "    data = cursor.fetchall()\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "    pandas_df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_deprecated_from_postgres(df, table_name='data_table', db_config=db_config):\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"key1\", \"1\", \"01\", \"2023-09-25 10:00:00\"),\n",
    "    (\"key2\", \"1\", \"01\", \"2023-09-25 11:00:00\"),\n",
    "    (\"key3\", \"1\", \"01\", \"2023-09-25 12:00:00\"),\n",
    "    (\"key4\", \"1\", \"01\", \"2023-09-25 12:00:00\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"key\", T.StringType(), True),\n",
    "    T.StructField(\"value\", T.StringType(), True),\n",
    "    T.StructField(\"year\", T.StringType(), True),\n",
    "    T.StructField(\"time\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_spark_df_to_postgres(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+-------------------+\n",
      "| key|value|year|               time|\n",
      "+----+-----+----+-------------------+\n",
      "|key1|    1|  01|2023-09-25 10:00:00|\n",
      "|key2|    1|  01|2023-09-25 11:00:00|\n",
      "|key3|    1|  01|2023-09-25 12:00:00|\n",
      "|key4|    1|  01|2023-09-25 12:00:00|\n",
      "+----+-----+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_postgres_to_spark_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"key1\", \"2\", \"01\", \"2023-09-25 10:00:00\"),\n",
    "    (\"key2\", \"2\", \"01\", \"2023-09-25 11:00:00\"),\n",
    "    (\"key3\", \"2\", \"01\", \"2023-09-25 12:00:00\"),\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"key\", T.StringType(), True),\n",
    "    T.StructField(\"value\", T.StringType(), True),\n",
    "    T.StructField(\"year\", T.StringType(), True),\n",
    "    T.StructField(\"time\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "save_spark_df_to_postgres(df)\n",
    "drop_deprecated_from_postgres(df)\n",
    "read_postgres_to_spark_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"key1\", \"3\", \"02\", \"2023-09-25 10:00:00\"),\n",
    "    (\"key2\", \"3\", \"02\", \"2023-09-25 11:00:00\"),\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"key\", T.StringType(), True),\n",
    "    T.StructField(\"value\", T.StringType(), True),\n",
    "    T.StructField(\"year\", T.StringType(), True),\n",
    "    T.StructField(\"time\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
